{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTJP3h3blcDm"
   },
   "source": [
    "# Recherche d'Information et traitement de données massives\n",
    "\n",
    "# Lab 5 : Recherche d’information sur le Web : passage à l'échelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHVZM46o59mY"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>IMPORTANT:</b> Assurez-vous que votre notebook utilise un kernel python 3 !\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3yAdUK5lcDp"
   },
   "source": [
    "  + Ce Lab s'intéresse au problème de passage à l'échelle des données du Web, à l'aide d'une approche reposant sur le paradigme de MapReduce. Vous allez notamment écrire un algorithme dans le cadre de MapReduce.\n",
    "\n",
    "\n",
    "\n",
    "## EXERCICES : écriture d'algorithmes en MapReduce\n",
    "### Avant propos : un bref rappel de MapReduce\n",
    "Comme nous l'avons vu dans le cours 5, Map Reduce est un **modèle de programmation** (ou patron de programmation) qui fournit un cadre pour automatiser le calcul distribué sur des données massives. Ce cadre propose d'écrire tout traitement à l'aide de deux opérations `map` et  `reduce` et de la représentation des données sous la forme de paires `(clé,valeur)`.\n",
    "MapReduce est aussi un **framework d'exécution** qui permet l'éxécution distribuée des programmes écrits selon ce cadre, et cela de manière totalement transparente selon le schéma rappelé ci-dessous.\n",
    "\n",
    "\n",
    "<img src=\"https://zupimages.net/up/21/18/iohi.png\" width=\"500\" height=\"500\" />\n",
    "\n",
    "1.  Un certain nombre de tâches *Map* sont alimentées par une ou\n",
    "    plusieurs partitions de données en provenance d'un système de\n",
    "    fichiers distribués (par exemple GFS, HDFS, S3).  Ces tâches *Map* transforment ces données en une séquence de paires clés-valeurs. C'est le développeur qui détermine comment sont calculées les paires\n",
    "    clés-valeurs en fonction des données en entrée en écrivant le code dans la fonction `map()`.\n",
    "2.  Les paires clés-valeurs sont collectées par un contrôleur maître et triées par clés.\n",
    "    Les paires sont redirigées vers les tâches *Reduce* de façon à ce que toutes les paires\n",
    "    qui ont la même clé soient redirigées vers la même tâche *Reduce*.\n",
    "3.  Les tâches *Reduce* traitent les clés une par une. Elles agrègent/combinent les valeurs associées\n",
    "    aux clés selon le code spécifié dans la fonction `reduce()`.\n",
    "\n",
    "\n",
    "\n",
    "Dans le cours, nous avons vu comment écrire en MapReduce un programme permettant de compter le nombre d'occurences des mots d'une collection donnée (programme **WordCount**). C'est une tâche qui peut sembler très simple à écrire quand on travaille sur une collection de petite taille. Vous avez d'ailleurs proposé une solution non-distribuée pour cette tâche pour la collection TIME (étape de filtrage par mots fréquents ou calcul de la pondération TF). En python, nous pouvons écrire cela très simplement, par exemple comme dans le programme ci-dessous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNjiI4lhlcDr"
   },
   "outputs": [],
   "source": [
    "def count_frequency(collection):\n",
    "    tokens_count={}\n",
    "    for doc_id in collection:\n",
    "        for token in collection[doc_id]:\n",
    "            if token in tokens_count.keys():\n",
    "                tokens_count[token]+=1\n",
    "            else:\n",
    "                tokens_count[token]=+1\n",
    "    return tokens_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCpHF-nmlcDz"
   },
   "source": [
    "Dans le cas où l'on considère une grosse collection dont le stokage et les traitements nécéssitent d'être distribués, alors cette tâche devient plus diffile à écrire, notamment car il faut prendre en compte la distribution des données et des traitements. Le modèle MapReduce apporte une solution à cela en proposant d'écrire ce type de programme selon le principe suivant :\n",
    "\n",
    "+ On considère que le document ou la partie du document est donné sous la forme d'une paire (clé,valeur) avec comme clé, l'identifiant du document et comme valeur le contenu textuel du document.\n",
    "\n",
    "+ Étant donnée une collection d’items, appliquer à chaque item un processus de transformation individuelle (étape `MAP`) qui produit des valeurs intermédiaires étiquetées. Dans le cas du WordCount, il s'agit juste de prendre chaque token du document ou de la partie du document et de la transformer en la paire (mot,1) comme illustré ci-dessous.\n",
    "\n",
    "<img src=\"https://zupimages.net/up/21/18/5cu8.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "\n",
    "+ Regrouper ces valeurs intermédiaires par étiquette (étape faite par le framework `SHUFFLE AND SORT`). On aura dans le cas du WordCount en sortie de cette étape en ensemble de paires (mot, [1,1,1,1..]) avec comme clés les différents mots du documents et comme valeur une liste des 1-occurence des mots dans le document considéré.\n",
    "+ Appliquer une fonction d'agrégation à chaque groupe (étape `REDUCE`).Dans le cas du Wordcount il s'agit juste de sommer la liste des occurences comme illustré ci-dessous.\n",
    "\n",
    "<img src=\"https://zupimages.net/up/21/18/dabe.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "\n",
    "### Concevoir un programme selon le cadre MapReduce : un peu de méthodologie\n",
    "\n",
    "Pour faciliter l'écriture de programmes selon le cadre MapReduce, il est souvent nécessaire de se poser d'abord  les questions ci-dessous.\n",
    "\n",
    " + De quelle nature sont les documents en entrée ? Comment les représenter sous une forme `(clé, valeur)` ?\n",
    " + Quelles sont les groupes visés ? Quelles sont les valeurs intermédiaires que je cherche à produire depuis mon document d'entrée ?\n",
    " + Quelle est la valeur finale ? Quelle est la nature de l'agrégation pour produire cette valeur finale ? \n",
    "\n",
    "Une fois les réponses à ces questions claires, il suffira ensuite :\n",
    " + D'écrire la fonction de `map` qui prend en entrée un document et qui produit une séquence de paires `(clé, valeur)`.\n",
    " + D'avoir en tête que ces différentes paires sont collectées par le framework et triées par clés pour donner une entrée à la tâche d'agrégation l'ensemble des paires qui ont la même clé. \n",
    " + D'écrire la fonction `reduce` qui prend en entrée une paire `(clé, liste(valeurs))` en spécifiant le traitement d'agrégation voulu. \n",
    "\n",
    "Le schéma d'éxécution et sa trace sur le problème du WordCount vous est donné ci-dessous, pour rappel.\n",
    "\n",
    "<img src=\"https://zupimages.net/up/21/18/otvx.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "<img src=\"https://zupimages.net/up/21/18/lsoz.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "Nous allons maintenant appliquer le cadre MapReduce au calcul de la pondération `TF-IDF`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqFtf1QR6hEb"
   },
   "source": [
    "Dans le cas où l'on considère une grosse collection dont le stokage et les traitements nécéssitent d'être distribués, alors cette tâche devient plus diffile à écrire, notamment car il faut prendre en compte la distribution des données et des traitements. Le modèle MapReduce apporte une solution à cela en proposant d'écrire ce type de programme selon le principe suivant :\n",
    "\n",
    "+ On considère que le document ou la partie du document est donné sous la forme d'une paire (clé,valeur) avec comme clé, l'identifiant du document et comme valeur le contenu textuel du document.\n",
    "\n",
    "+ Étant donnée une collection d’items, appliquer à chaque item un processus de transformation individuelle (étape `MAP`) qui produit des valeurs intermédiaires étiquetées. Dans le cas du WordCount, il s'agit juste de prendre chaque token du document ou de la partie du document et de la transformer en la paire (mot,1) comme illustré ci-dessous.\n",
    "\n",
    "<img src=\"https://zupimages.net/up/21/18/5cu8.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "\n",
    "+ Regrouper ces valeurs intermédiaires par étiquette (étape faite par le framework `SHUFFLE AND SORT`). On aura dans le cas du WordCount en sortie de cette étape en ensemble de paires (mot, [1,1,1,1..]) avec comme clés les différents mots du documents et comme valeur une liste des 1-occurence des mots dans le document considéré.\n",
    "+ Appliquer une fonction d'agrégation à chaque groupe (étape `REDUCE`).Dans le cas du Wordcount il s'agit juste de sommer la liste des occurences comme illustré ci-dessous.\n",
    "\n",
    "<img src=\"https://zupimages.net/up/21/18/dabe.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "\n",
    "### Concevoir un programme selon le cadre MapReduce : un peu de méthodologie\n",
    "\n",
    "Pour faciliter l'écriture de programmes selon le cadre MapReduce, il est souvent nécessaire de se poser d'abord  les questions ci-dessous.\n",
    "\n",
    " + De quelle nature sont les documents en entrée ? Comment les représenter sous une forme `(clé, valeur)` ?\n",
    " + Quelles sont les groupes visés ? Quelles sont les valeurs intermédiaires que je cherche à produire depuis mon document d'entrée ?\n",
    " + Quelle est la valeur finale ? Quelle est la nature de l'agrégation pour produire cette valeur finale ? \n",
    "\n",
    "Une fois les réponses à ces questions claires, il suffira ensuite :\n",
    " + D'écrire la fonction de `map` qui prend en entrée un document et qui produit une séquence de paires `(clé, valeur)`.\n",
    " + D'avoir en tête que ces différentes paires sont collectées par le framework et triées par clés pour donner une entrée à la tâche d'agrégation l'ensemble des paires qui ont la même clé. \n",
    " + D'écrire la fonction `reduce` qui prend en entrée une paire `(clé, liste(valeurs))` en spécifiant le traitement d'agrégation voulu. \n",
    "\n",
    "Le schéma d'éxécution et sa trace sur le problème du WordCount vous est donné ci-dessous, pour rappel.\n",
    "\n",
    "<img src=\"https://zupimages.net/up/21/18/otvx.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "<img src=\"https://zupimages.net/up/21/18/lsoz.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "Nous allons maintenant appliquer le cadre MapReduce au calcul de la pondération `TF-IDF`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4vEyqnw6cxu"
   },
   "source": [
    "Dans le cas où l'on considère une grosse collection dont le stokage et les traitements nécéssitent d'être distribués, alors cette tâche devient plus diffile à écrire, notamment car il faut prendre en compte la distribution des données et des traitements. Le modèle MapReduce apporte une solution à cela en proposant d'écrire ce type de programme selon le principe suivant :\n",
    "\n",
    "+ On considère que le document ou la partie du document est donné sous la forme d'une paire (clé,valeur) avec comme clé, l'identifiant du document et comme valeur le contenu textuel du document.\n",
    "\n",
    "+ Étant donnée une collection d’items, appliquer à chaque item un processus de transformation individuelle (étape `MAP`) qui produit des valeurs intermédiaires étiquetées. Dans le cas du WordCount, il s'agit juste de prendre chaque token du document ou de la partie du document et de la transformer en la paire (mot,1) comme illustré ci-dessous.\n",
    "\n",
    "<img src=\"https://zupimages.net/up/21/18/5cu8.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "\n",
    "+ Regrouper ces valeurs intermédiaires par étiquette (étape faite par le framework `SHUFFLE AND SORT`). On aura dans le cas du WordCount en sortie de cette étape en ensemble de paires (mot, [1,1,1,1..]) avec comme clés les différents mots du documents et comme valeur une liste des 1-occurence des mots dans le document considéré.\n",
    "+ Appliquer une fonction d'agrégation à chaque groupe (étape `REDUCE`).Dans le cas du Wordcount il s'agit juste de sommer la liste des occurences comme illustré ci-dessous.\n",
    "\n",
    "<img src=\"https://zupimages.net/up/21/18/dabe.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "\n",
    "### Concevoir un programme selon le cadre MapReduce : un peu de méthodologie\n",
    "\n",
    "Pour faciliter l'écriture de programmes selon le cadre MapReduce, il est souvent nécessaire de se poser d'abord  les questions ci-dessous.\n",
    "\n",
    " + De quelle nature sont les documents en entrée ? Comment les représenter sous une forme `(clé, valeur)` ?\n",
    " + Quelles sont les groupes visés ? Quelles sont les valeurs intermédiaires que je cherche à produire depuis mon document d'entrée ?\n",
    " + Quelle est la valeur finale ? Quelle est la nature de l'agrégation pour produire cette valeur finale ? \n",
    "\n",
    "Une fois les réponses à ces questions claires, il suffira ensuite :\n",
    " + D'écrire la fonction de `map` qui prend en entrée un document et qui produit une séquence de paires `(clé, valeur)`.\n",
    " + D'avoir en tête que ces différentes paires sont collectées par le framework et triées par clés pour donner une entrée à la tâche d'agrégation l'ensemble des paires qui ont la même clé. \n",
    " + D'écrire la fonction `reduce` qui prend en entrée une paire `(clé, liste(valeurs))` en spécifiant le traitement d'agrégation voulu. \n",
    "\n",
    "Le schéma d'éxécution et sa trace sur le problème du WordCount vous est donné ci-dessous, pour rappel.\n",
    "\n",
    "<img src=\"https://zupimages.net/up/21/18/otvx.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "<img src=\"https://zupimages.net/up/21/18/lsoz.jpg\" width=\"500\" height=\"500\" />\n",
    "\n",
    "Nous allons maintenant appliquer le cadre MapReduce au calcul de la pondération `TF-IDF`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D07OSBCklcD1"
   },
   "source": [
    "##  Exercice 1 : Tf-IDF en MapReduce\n",
    "\n",
    "L'objectif est de calculer `Tf-IDF` pour un ensemble de documents en utilisant le modèle MapReduce. Pour rappel, comme vu dans le cours 1, `Tf-IDF (Term frequency-Inverse Document Frequency)` est une statistique qui traduit le niveau d'importance d'un terme $t$ pour un document $d$ appartenant à une collection (ou un corpus) de taille $N$. Dans cet exercice, on considèrera la formulation mathématique suivante :\n",
    "\n",
    "$$TF-IDF(t_i,d) = ( \\frac{tf_{t_i,d}}{\\sum_{t_k \\in d} tf_{t_k,d}}  ) \\times \\log \\left( \\frac{N}{df_t}\\right)$$\n",
    "\n",
    "où $tf_{t,d}$ est le nombre d'occurrence du terme $t$ dans le document $d$, $N$ le nombre de documents de la collection et $df_t$ le nombre de documents dans lesquels $t$ est présent.\n",
    "\n",
    "Vous pouvez prendre le temps de regarder la correction du Lab2 dans lequel nous avons proposé des solutions pour calculer cette statistique sur la collection TIME.\n",
    "\n",
    "Pour calculer cette statistique sur une très grosse collection de documents, il est nécéssaire de distribuer son calcul. Nous allons pour cela découper le travail en 3 étapes :\n",
    "\n",
    " + **1- Le calcul, pour chaque mot, de son nombre d'occurences par document.** Nous appelerons cette étape `WordFrequenceInDocs`.\n",
    " + **2- Le calcul du nombre de mots par documents**. Nous appelerons cette étape `WordCountsForDocs`.\n",
    " + **3- La combinaison des 2 informations précédentes pour calculer le TF_IDF**. Nous appelerons cette étape `WordsInCorpusTFIDF`.\n",
    " \n",
    "#### Etape 1 : WordFrequenceInDocs\n",
    "\n",
    "Il s'agit donc ici de formuler le problème du calcul du nombre d'occurences des mots d'un document avec le cadre MapReduce. C'est un problème très proche du problème du WordCount vu en cours et rappelé ci-dessus et vous devriez pouvoir y répondre rapidement.\n",
    "\n",
    "**Données d'entrée**\n",
    "\n",
    "Votre premier travail est de proposer une représentation adéquate de vos données d'entrée (une collection de document) pour le cadre MapReduce. On considère que chaque tâche `MAP` traitera un seul document (ou partie de document) et c'est donc ce document (ou cette partie) qui sera pris comme entrée de la fonction `MAP`.\n",
    "Que proposez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TwY_gfAlcD3"
   },
   "outputs": [],
   "source": [
    "# A compléter \n",
    "\n",
    "Input  : ...  #ce qu'on a\n",
    "Output : ...   #ce qu'on veut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r63CLqfulcD8"
   },
   "source": [
    "**Fonction MAP**\n",
    "\n",
    "En vous inspirant du WordCount, écrire en pseudo-code, la fonction MAP pour cette étape. Attention, il faudra pouvoir garder l'information relative à l'identifiant du document considéré.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8-RW8iflcD9"
   },
   "outputs": [],
   "source": [
    "# A compléter \n",
    "\n",
    "Fonction MAP:\n",
    "\n",
    "... pseudo code ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp3c-mGRlcEB"
   },
   "source": [
    "**Fonction REDUCE**\n",
    "\n",
    "L'ensemble des paires (clé,valeur) provenant des différents noeuds `MAP` sont collectées et triées par clés intermédiaires et donc fournies sous cette forme à la tâche `REDUCE`. Ecrire, en pseudo-code la fonction `REDUCE` pour `WordFrequenceInDocs`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uYlPMXxwlcEE"
   },
   "outputs": [],
   "source": [
    "# A compléter \n",
    "\n",
    "Fonction REDUCER:\n",
    "\n",
    "Input : ... #ce qu'on a\n",
    "Output: ... #ce qu'on veut \n",
    "\n",
    "... pseudo code ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CY9qUu7elcEJ"
   },
   "source": [
    "#### Etape 2: WordCountsForDocs\n",
    "\n",
    "Il s'agit maintenant de calculer le nombre de mots par documents. Deux petites indications :\n",
    "+ L'entrée de cette tâche sera la sortie de la tâche précédente soit une paire de type \n",
    "`((word,doc_id),n)` avec `n` le nombre d'occurence du terme word dans le document `doc_id` soit le tf.\n",
    "+ Pour cette tâche, il pourrait être intéressant de pouvoir avoir accès aux documents par le mécanisme (clé,valeur).\n",
    "\n",
    "Ecrire, en pseudo-code, la fonction `MAP`, puis la fonction `REDUCE` vous permettant de faire cette étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EMuX_wXjlcEK"
   },
   "outputs": [],
   "source": [
    "# A compléter\n",
    "\n",
    "Fonction MAP:\n",
    "Input: ...\n",
    "Output: ...\n",
    "    \n",
    "... pseudo code ....\n",
    "    \n",
    "    \n",
    "Fonction REDUCER:\n",
    "Input: ...\n",
    "Output: ...\n",
    "    \n",
    "... pseudo code ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwO0KHntlcEQ"
   },
   "source": [
    "#### Etape 3: WordsInCorpusTFIDF\n",
    "\n",
    "Il s'agit maintenant de combiner les informations précédentes pour calculer le TF-IDF pour chaque terme. On considère à nouveau ici que l'entrée est la sortie de la tâche précédente soit une paire de type \n",
    "`((word,doc_id),n/N)` avec `n` le nombre d'occurence du terme word dans le document `doc_id` et `N` le nombre total de mots dans le document `doc_id`. On supposera aussi que le nombre de documents $D$ dans la collection est passé au système sous la forme d'une constante.\n",
    "\n",
    "Ecrire, en pseudo-code, la fonction `MAP`, puis la fonction `REDUCE` vous permettant de faire cette étape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3TNT6rklcES"
   },
   "outputs": [],
   "source": [
    "# A compléter\n",
    "\n",
    "Fonction MAP:\n",
    "Input: ...\n",
    "Output: ...\n",
    "\n",
    "... pseudo code ....\n",
    "    \n",
    "    \n",
    "Fonction Reducer:\n",
    "Input: ...\n",
    "Output: ...\n",
    "\n",
    "... pseudo code ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wOI-1LqlcEV"
   },
   "source": [
    "## Exercice 2 : Construction d'un index inversé en MapReduce\n",
    "\n",
    "Il s'agit ici de refléchir à comment un index inversé de documents peut être construit de manière distribuée à l'aide de MapReduce. \n",
    "\n",
    "1. Ecrivez en pseudo-code la fonction `MAP` en précisant bien le type des données d'entrée.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgE8CAITlcEW"
   },
   "outputs": [],
   "source": [
    "# A compléter\n",
    "\n",
    "Input: ...\n",
    "Output: ...\n",
    "\n",
    "Fonction MAP:\n",
    "\n",
    "... pseudo code ....\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HXQ3WwrlcEa"
   },
   "source": [
    "2. Ecrivez en pseudo-code la fonction `REDUCE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XszSQNXXlcEc"
   },
   "outputs": [],
   "source": [
    "# A compléter\n",
    "\n",
    "Fonction REDUCER:\n",
    "\n",
    "Input : ...\n",
    "Output: ...\n",
    "\n",
    "\n",
    "... pseudo code ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjucRujolcEf"
   },
   "source": [
    "3. Un test sur machine\n",
    "\n",
    "Pour vous permettre de mettre en oeuvre ce mécanisme de manière concrète, vous allez appliquer cela à l'indexation d'une collection [books.json](../Data/books.json).\n",
    "On cherche à produire un fichier inversé qui indique pour chaque mot, la liste des livres dans lesquels il apparaît en utilisant le cadre MapReduce.\n",
    "\n",
    "Pour cela, nous vous fournissons dans le répertoire [Utils](./Utils) un fichier [Lab4.py](./Utils/Lab4.py) qui contient un ensemble de fonctions qui vous seront utiles.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpIj2RZ7lcEg"
   },
   "source": [
    "Importer les fonctions utiles de ce module python à l'aide de la commande ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UOvVpRPlcEi"
   },
   "outputs": [],
   "source": [
    "from Utils.Lab5 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cunk9SnklcEl"
   },
   "source": [
    "A l'aide de la fonction `readData(filename)` de ce module, charger et afficher le contenu du fichier [books.json](../Data/books.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYet4x_YlcEn"
   },
   "outputs": [],
   "source": [
    "# A compléter\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkKMV_E9lcEq"
   },
   "source": [
    "Ecrire la fonction `mapper` nécessaire à la construction de l'index inversé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wc-TZ3OklcEr"
   },
   "outputs": [],
   "source": [
    "# A compléter\n",
    "\n",
    "def mapper(data):\n",
    "    ... code pyhton ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irozkNzrlcEw"
   },
   "source": [
    "Ecrire la fonction `reducer` nécessaire à la construction de l'index inversé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSsoq9SflcEx"
   },
   "outputs": [],
   "source": [
    "# A compléter\n",
    "\n",
    "def reducer(data): \n",
    "    ... code pyhton ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A88IjY5qlcEz"
   },
   "source": [
    "Tester vos deux fonctions en appliquant le code ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqcKWR75lcE0"
   },
   "outputs": [],
   "source": [
    "def invertedIndexExample(filename):\n",
    "    m = MapReduce(mapper, reducer)\n",
    "    results = m(readData(filename))\n",
    "    for w, b in results:\n",
    "        print(\"mot : \", w, \"livres : \", b)\n",
    "    return\n",
    "    \n",
    "invertedIndexExample('./Data/books.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emxKT1gE59mm"
   },
   "source": [
    "## Exercice 3 : Requête SQL en MapReduce (optionnel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uN6dt9wp59mm"
   },
   "source": [
    "On cherche à reproduire la requête :\n",
    "\n",
    "    SELECT *\n",
    "    FROM Orders, LineItem\n",
    "    WHERE Order.order_id = LineItem.order_id;\n",
    "\n",
    "Le fichier de données est [records.json](./Data/records.json). Vous devez réaliser la jointure sur le numéro de commande qui aparaît en colonne 1.\n",
    "Vous devez écrire les fonctions `mapper()` et `reducer()` :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKFxdyGk59mm"
   },
   "source": [
    "\n",
    "Ecrire la fonction `mapper` nécessaire à la préparation des données pour la jointure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "603_kLs959mm"
   },
   "outputs": [],
   "source": [
    "# A compléter\n",
    "\n",
    "def mapper2(data):\n",
    "    ... code python ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSWIyXEn59mm"
   },
   "source": [
    "\n",
    "Ecrire la foncion `reducer2` nécessaire à l'agégation des résultats :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8iKX40tb59mm"
   },
   "outputs": [],
   "source": [
    "# A compléter\n",
    "\n",
    "def reducer2(data): \n",
    "    ... code pyhton ..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "cKeMsCCc59mm"
   },
   "source": [
    "L'exécution sur le fichier de test : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMCKzEaK59mn"
   },
   "outputs": [],
   "source": [
    "def joinExample(filename):\n",
    "    m = MapReduce(mapper2, reducer2)\n",
    "    results = m(readData(filename))\n",
    "    results = itertools.chain.from_iterable(results)\n",
    "    for r in results:\n",
    "        print(r)\n",
    "    return\n",
    "\n",
    "joinExample('./Data/records.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLFvcDvP59mn"
   },
   "source": [
    "doit produire les lignes suivantes :\n",
    "\n",
    "    ['order', '1', '36901', 'O', '173665.47', '1996-01-02', '5-LOW', 'Clerk#000000951', '0', \n",
    "     'nstructions sleep furiously among ', 'line_item', '1', '155190', '7706', '1', '17', '21168.23', \n",
    "     '0.04', '0.02', 'N', 'O', '1996-03-13', '1996-02-12', '1996-03-22', 'DELIVER IN PERSON', \n",
    "     'TRUCK', 'egular courts above the']\n",
    "    ['order', '1', '36901', 'O', '173665.47', '1996-01-02', '5-LOW', 'Clerk#000000951', '0', \n",
    "     'nstructions sleep furiously among ', 'line_item', '1', '67310', '7311', '2', '36', '45983.16', \n",
    "     '0.09', '0.06', 'N', 'O', '1996-04-12', '1996-02-28', '1996-04-20', 'TAKE BACK RETURN', \n",
    "     'MAIL', 'ly final dependencies: slyly bold ']\n",
    "    ['order', '1', '36901', 'O', '173665.47', '1996-01-02', '5-LOW', 'Clerk#000000951', '0', \n",
    "     'nstructions sleep furiously among ', 'line_item', '1', '63700', '3701', '3', '8', '13309.60', \n",
    "     '0.10', '0.02', 'N', 'O', '1996-01-29', '1996-03-05', '1996-01-31', 'TAKE BACK RETURN', \n",
    "     'REG AIR', 'riously. regular, express dep']\n",
    "    ...\n",
    "\n",
    "Vous remarquerez que la fonction `mapper()` doit toujours retourner une liste. Ceci est dû à la structure de la classe MapReduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gu0REvQu59mn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab5_MapReduce-Student.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
